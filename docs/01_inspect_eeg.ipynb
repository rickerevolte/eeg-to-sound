{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEG to Sound Project\n",
    "\n",
    "![](https://rickerevolte.de/favicon.png)\n",
    "\n",
    "This work in progress-project aims to extract information from EEG-Files to use them as a modifiers for sound emulating e.g. sythesizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a test on an unknown EEG File that has been delivered as a raw binary without nearly any supplemantary informations.\n",
    "First, make sure you are runnung python 3 and install all necessary packages via pip.\n",
    "This jupyter book is optimized for python 3.9.2\n",
    "I recommend creating a virtual environment for this.\n",
    "Need help?\n",
    "https://packaging.python.org/en/latest/tutorials/installing-packages/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do some ASCII-Sniffing in order to obtain Information out of our demofile's header. Let's set the path to the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../demo_EEG/demofile.EEG\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our demofile's size. We will need that size later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, \"rb\") as f:\n",
    "    data = f.read()\n",
    "    print(f\"{path} has the size of {len(data)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines the function seek_ascii() with the 2 arguments path (string) and number (int) of bytes we want to seek for readable ascii code.\n",
    "We will then call that function later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seek_ascii(path, n):\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = f.read(n)\n",
    "    printable = []\n",
    "    cur = bytearray()\n",
    "    for b in data:\n",
    "        if 32 <= b <= 126:\n",
    "            cur.append(b)\n",
    "        else:\n",
    "            if len(cur) >= 4:\n",
    "                try:\n",
    "                    printable.append(cur.decode(\"ascii\", errors=\"ignore\"))\n",
    "                except:\n",
    "                    pass\n",
    "            cur = bytearray()\n",
    "    if len(cur) >= 4:\n",
    "        try:\n",
    "            printable.append(cur.decode(\"ascii\", errors=\"ignore\"))\n",
    "        except:\n",
    "            pass\n",
    "    return printable, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will only check the header. For this we are reading the first few bytes. You can change that amount by giving n another value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, \"rb\") as f:\n",
    "    data = f.read(n)\n",
    "    ascii_blocks, raw_head = seek_ascii(path, n=8192)\n",
    "    print(f\"\\nReadable ASCII blocks found (first {n} Bytes):\")\n",
    "    if not ascii_blocks:\n",
    "        print(\"  (Readable ASCII blocks found)\")\n",
    "    else:\n",
    "        for s in ascii_blocks:\n",
    "            print(\" -\", s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, we can see some readable information in our demo file, such as names and dates, but not the important information we need to read our demo file correctly.\n",
    "The missing information is:\n",
    "Samplingrate, Number and order of channels\n",
    "Data type (continuous, event-based)\n",
    "Byte order (little-endian/big-endian)\n",
    "and more.\n",
    "In the following, we will take steps to obtain as much accurate information as possible in order to read, visualize, and continue working with our EEG file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to draw conclusions about the number of channels based on the file size, the approximate examination time, and various standard sizes as well as different types of data, e.g. 16 bit, 32 bit per sample.\n",
    "\n",
    "Our numerator is the size of our demofile.\n",
    "Our denominator will then be: number of channels * number of bytes per sample * samplingrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_duration(filesize, n_channels=(), dtype_bytes_options=()):\n",
    "    print(\"\\nduration estimation (for different bytes/per sample):\")\n",
    "    for nch in n_channels:\n",
    "        for bps in dtype_bytes_options:\n",
    "            seconds = filesize / (nch * bps * 256.0)  # using 256 Hz as example\n",
    "            mm = int(seconds // 60)\n",
    "            ss = int(seconds % 60)\n",
    "            print(f\" - number of channels={nch} / bytes/sample={bps}: {seconds:.2f} s  â†’ {mm} min {ss} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume different numbers of channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels=(19,21,23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and different byte options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_bytes_options=(2,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And will run the function guess_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, \"rb\") as f:\n",
    "    data = f.read()\n",
    "    filesize = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_duration(filesize, n_channels, dtype_bytes_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately our patient remembers a rough time estimation of ca 20 minutes and made a selfie during the examination in which one can see the EEG amplifier with 23 Elektrodes. Since the device has two neutral elektrodes named \"NE\" as well as two mastoides \"A1\" and \"A2\", that are only used for referencing, the result is a classical 10-20 EEG with 19 channels.\n",
    "Regarding the duration estimation with different numbers of channels and bytes per sample, we can draw the conclusion of 19 channels, and two bytes per sample with a samplingrate of 256 Hz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function endian_test will run some tests on our demofile in order to find out the most possible byte-order little- or big-endian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def endian_test(path, n_channels=(), dtype='', n_samples_to_read=()):\n",
    "    dt_size = np.dtype(dtype).itemsize\n",
    "    bytes_needed = n_channels * n_samples_to_read * dt_size\n",
    "    filesize = os.path.getsize(path)\n",
    "    if filesize < bytes_needed:\n",
    "        n_samples_to_read = max(1, filesize // (n_channels * dt_size))\n",
    "        bytes_needed = n_channels * n_samples_to_read * dt_size\n",
    "    print(f\"\\nendianness/statistical test: read first {n_samples_to_read} samples per channel (total {bytes_needed} Bytes).\")\n",
    "    with open(path, \"rb\") as f:\n",
    "        raw = f.read(bytes_needed)\n",
    "    arr_le = np.frombuffer(raw, dtype='<i2')  # little-endian int16\n",
    "    arr_be = np.frombuffer(raw, dtype='>i2')  # big-endian int16\n",
    "    # try to reshape assuming interleaved samples (time major)\n",
    "    for name, arr in ((\"little-endian\", arr_le), (\"big-endian\", arr_be)):\n",
    "        if arr.size % n_channels != 0:\n",
    "            print(f\"  {name}: not divisible by {n_channels} (len={arr.size})\")\n",
    "            continue\n",
    "        arr2 = arr.reshape((-1, n_channels)).T  # shape (n_channels, n_times)\n",
    "        mins = arr2.min(axis=1)\n",
    "        maxs = arr2.max(axis=1)\n",
    "        means = arr2.mean(axis=1)\n",
    "        stds = arr2.std(axis=1)\n",
    "        print(f\"  {name}: samples total {arr.size}, per channel {arr2.shape[1]}\")\n",
    "        print(f\"    channel-min (first 5): {mins[:5].tolist()}\")\n",
    "        print(f\"    channel-max (first 5): {maxs[:5].tolist()}\")\n",
    "        print(f\"    channel-mean (first 5): {[round(x,2) for x in means[:5]]}\")\n",
    "        print(f\"    channel-std  (first 5): {[round(x,2) for x in stds[:5]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "endian_test(path, n_channels=19, dtype='int16', n_samples_to_read=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Endianness statistical test shows for little endian a coverage of the full range of values of 2 powers 16 = 65536 values (-32768 to 32767) and a standard deviation of roundabout 18.000. The statistics for big endian are much smaller and therefore it is rather unlikely that we are dealing with big-endian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, just for demonstration, this section of code is the fully working, but still we are missing an important information, which is the correct order of the channels. Up to this point, we have applied standards that do not necessarily apply.\n",
    "Before we start looking for typical neurological responses to trigger events in another tutorial in order to assign the neurological responses to the corresponding stimulated brain areas, we will now take a detour to analyze the change in frequency over time.\n",
    "\n",
    "To be continued\n",
    "(Last updated: November 2025)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 (ipykernel) - find me in '~/.pyenv/shims/python'",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
